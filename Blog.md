# DapperFL: Domain Adaptive Federated Learning with Model Fusion Pruning for Edge Devices

## Overview

Federated Learning has emerged as a prominent and vital machine learning paradign in edge computing environments where edge devices collaborately train machine learning models without sharing raw data. As FL does not require raw data sharing, privacy of each edge devices can be preserved but it struggles with challenges like system heterogeneity and domain shifts. **DapperFL** attempts to solve this issue which is a novel FL framework tailored for heterogeneous edge environments and robust to domain shifting. In this blog, we explore how **DapperFL** applies unconventional and innovative techniques like **Model Fusion Pruning (_MFP_)** and **Domain Adaptive Regularization (_DAR_**) to reach a solution for the abovementioned two issues.

## Problem

Federated Learning empowers edge devices to collaborate on building a global model without compromising privacy. However, two core challenges often undermine its effectiveness:

- **System Heterogeneity:** In the prevelent FL environments, participent clients generally have diverse and in some cases constrained system configurations which often results in low-capability clients failing to complete local training of FL. Consequently, It diminishes the performance of the aggregated global model as less capable devices struggle participate effectively
- **Domain Shift:** As FL is distributed in nature, the training data among the clients is significantly diverse. As data distributions vary a lot, these different domains for each clients lead to diverse local models. with non-IID data, global model aggregation tends to become biased towards the clients which contribute most data, thus eventually hindering the training impact of other clients.

## DapperFL Solution

DapperFL introduces a robust framework which comprises two key modules, namely Module Fusion Pruning (MFP) and Domain Adaptive Regularization (DAR).

#### Model Fusion Pruning

MFP module is designed to compress the local model while simultenously addressing the domain shift problems

###### Method of Model Fusion Pruning

1. In the initial epoch of each communication round $t \in [1,T]$, The MFP module first fine-tunes the global model $\mathcal{W}^{t-1} $ on local data to produce local model $\hat{w}^t_i$
2. Then MFP Module fuses the global model $\mathcal{W}_{t-1}$ into the local model $\hat{w}^t_i$ to embed the cross-domain information into local model which is formulated by $w^t_i = \alpha^t \mathcal{W}^{t-1} + (1 - \alpha)\hat{w}^t_i$.
3. Here $\alpha^t$ is dynamically adjusted. This dynamic adjusting mechanism is described as $\alpha^t = max \{{(\epsilon -1 )^{t-1}\alpha_0, \alpha_{min}} \}$
4. Subsequently, the MFP module calculates a binary mask matrix $M_i^t \in \{ 0, 1\}^{|w^t_i|}$ for pruning the local model $w^t_i$ . The matrix $M$ is derived through the channel wise $l_1$ norm.

#### Domain Adaptive Regularization

the DAR module employs regularization generated by the compressed model to further mitigate domain shift problems and hence improve the overall performance of DapperFL. Specifically, the DAR module introduces a regularization term to the local objective to alleviate the bias of representations on each clients adaptively

###### Method of Domain Adaptive Regularization

1. We first segment the pruned local model $w \odot M$ into two parts e.g. encoder $w_e \odot M_e$ and a predictor $w_p \odot M_p$
2. Then we contruct a regularization term $\mathcal{L}^{DAR} = || g_e(w_e \odot M_e; x_i) ||^2_2$ which is based on $l_2$ norm
3. Next, the cross entropy loss used in DAP is constructed as $\mathcal{L}_i^{CE} = - \frac{1}{\mathcal{K_i}} \sum_{\mathcal{k} \in \mathcal{K}_i} y_{i,k} \space log(\hat{y}_{i,k} )$ where $\mathcal{K}_i$ denotes the set of possible labels in client $i$
4. Finally, the training objective is set as $\mathcal{L}_i = \mathcal{L}_i^{CE} + \gamma \mathcal{L}_i^{DAR}$

#### Heterogenous Model Aggregation

To preserve specific domain knowledge while transferring global knowledge to the local model, the central server first recovers the structure of local models before aggregating. The local model is recovered as follows $w_i^t = \hat{w}_i^t \odot M_i^t + \mathcal{W}^{t-1}\odot \overline{M}$

Finally, the global model is calculated by aggregating the recovered local models as follows: $\mathcal{W}^t = \sum \frac{|\mathcal{D_i}|}{|\mathcal{D}|} w_i^t$

## Results and Findings

DapperFL was implemented on a real-world FL platform using PyTorch and evaluated on two domain generalization benchmarks:\***\*Digits** (MNIST, USPS, SVHN, SYN) and\*\* **Office Caltech** (Caltech, Amazon, Webcam, DSLR).

#### Key Findings

###### Superior Accuracy

DapperFL outperformed the highest global accuracy across all the benchmarks compared to state-of-the-art frameworks. such as:

- On the **Digital benchmark ,** DapperFL recorded a global accuracy of 74.30% outperforming every other frameworks by at least **2.28%**
- On the **Office Caltech benchmark,** DapperFL recorded a global accuracy of 67.75% beating the runner-up framework by **2.28%**

These findings illustrate DapperFL's ability to generalize efficiently and effectively even if the domain is diverse

###### Individual Domain Accuracy

DapperFL consistently ranked among the state-of-the-art frameworks for domain accuracy

- **SYN Domain:** 37.26% which was top accuracy
- **Amazon Domain:** 81.58% which was top accuracy
- **DSLR Domain:** 66.67% which matched the highest accuracy

###### Adaptability to Heterogeneous Clients

By employing MFP module, DapperFL reduced model sizes by 20% to 80% which enabled low-configured clients to participate efficiently. It also maintained competitive accuracy outperforming other frameworks such as **FedMP, NeFL**

## My Interpretation

DapperFL's approach to solution for the heterogeneity and domain shifts issue is a significant step forward in federated learning. The MFP module strikes a fine balance between model efficiency and performance. Simultenously, DAR module ensures that models generalize even when the domains are diverse which is crucial for real-world applications.

## Applications

- **Decentralized Healthcare Systems:** Hospitals and clinics will not be needing to share sensitive patient data across diverse patient demographics
- **IoT Networks:** Devices with varying computational capabilities can contribute to the global model which will enhace traffic management and environmental monitoring
- **Federated Autonomous Systems:** Autonomous vehicles can train models that will adapt to different

## Future Directions

Despite its potential in addressing system heterogeneity and domain shifts, DapperFL introduces four hyper-parameters $\alpha_0$, $\alpha_{min}$, $\epsilon$, and $\gamma$ associated with the DG
performance of the global model. A potential future direction involves the automatic selection of these hyper-parameters, therefore enhancing the flexibility and accessibility of the DapperFL.
